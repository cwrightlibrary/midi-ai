{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71231a3a",
   "metadata": {},
   "source": [
    "# midiAI\n",
    "\n",
    "In this notebook, I'll go through my process of using `music21` to create lists of information about songs generated from the `.midi` files in `data/` generated from the [*OpenBook*](https://veltzer.github.io/openbook/) repository.\n",
    "\n",
    "Let's begin by importing our libraries. We're going to be using `midi2audio` to playback our generated `.wav`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "803fd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from midi2audio import FluidSynth\n",
    "from music21 import converter, harmony, pitch, chord, key, interval, note, stream\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6293c",
   "metadata": {},
   "source": [
    "## `.midi` file playback\n",
    "\n",
    "Let's get our playback system running by defining our `FluidSynth` using the `gm-soundfont.sf2` from the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b12efc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FluidSynth(sound_font=\"assets/gm-soundfont.sf2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ef8eb",
   "metadata": {},
   "source": [
    "Let's make it easier to create `.wav` files for playback with this quick `.midi` to `.wav` function that creates the `.wav` file and `return`s the output filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4a958",
   "metadata": {},
   "source": [
    "We're also going to need to hide the wildly long fluidsynth warning outputs, so we'll do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d061f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_midi(mid_path):\n",
    "\toutput_dir = Path(\"out\")\n",
    "\toutput_dir.mkdir(exist_ok=True)\n",
    "\toutput_name = output_dir / Path(mid_path).with_suffix(\".wav\").name\n",
    "\t\n",
    "\t# fs.midi_to_audio(mid_path, output_name)\n",
    "\tsubprocess.run(\n",
    "\t\t[\"fluidsynth\", \"-ni\", \"gm-soundfont.sf2\", str(mid_path), \"-F\", str(output_name), \"-r 22050\", \"-q\"],\n",
    "\t\tstdout=subprocess.DEVNULL,\n",
    "\t\tstderr=subprocess.DEVNULL\n",
    "\t)\n",
    "\n",
    "\treturn output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4feaae",
   "metadata": {},
   "source": [
    "As a test, we're going to work with `solar.midi` from `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "42c32502",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_path = \"data/solar.midi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed296",
   "metadata": {},
   "source": [
    "Now we can give our `play_midi` function a go and make sure it's working. If you're following along with this notebook, make sure `fluidsynth` is installed using `brew install fluidsynth` if on a Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc8efd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(play_midi(solar_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a948e3",
   "metadata": {},
   "source": [
    "## `.music21` parsing\n",
    "\n",
    "Let's get into using `music21` to sort through our `.midi` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "77e1375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar = converter.parse(solar_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade8960",
   "metadata": {},
   "source": [
    "We'll inevitably come into problems trying to analyze the `harmony.chordSymbolFromChord` when `music21` can't figure out what the chord symbol should be because there are sharp symbols instead of flat symbols, so let's prepare a couple of `dict`s to replace those notes if we come across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5a397647",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_replacements = {\n",
    "\t\"A#\": \"B-\", \"C#\": \"D-\", \"D#\": \"E-\", \"F#\": \"G-\", \"G#\": \"A-\"\n",
    "}\n",
    "\n",
    "note_replacements_reverse = {\n",
    "\t\"B-\": \"A#\", \"D-\": \"C#\", \"E-\": \"D#\", \"G-\": \"F#\", \"A-\": \"G#\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bc817",
   "metadata": {},
   "source": [
    "### `chord_measures`\n",
    "\n",
    "Now let's get into creating lists of chord names and note information from each measure. The chords will always be in the first part of the parsed `.midi` file (index 0) and the notes will always be the second part (index 1).\n",
    "\n",
    "We'll then create empty lists that will hold each measure of chord(s) and note information. You'll notice I had to replace a few sharp notes with flats in order to get the correct chords as well as completely replacing the chord name for a G7b9, which was showing up as G7addG# (essentially the same thing).\n",
    "\n",
    "### `note_measures`\n",
    "\n",
    "The note information will be stored in the `note_measures` list where each note is a list with the note's *name* and *octave* in one string and the length of the note (in terms of `music21`'s `quarterNote`) in the next element of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f5689132",
   "metadata": {},
   "outputs": [],
   "source": [
    "chords = solar.parts[0]\n",
    "melody = solar.parts[1]\n",
    "\n",
    "chord_measures = []\n",
    "for item in chords:\n",
    "\tmeasure = []\n",
    "\tfor c in item:\n",
    "\t\tif \"chord.Chord\" in str(c):\n",
    "\t\t\tcs = harmony.chordSymbolFromChord(c)\n",
    "\t\t\tif \"Cannot\" in cs.figure:\n",
    "\t\t\t\tpitches = [p for p in c.pitches]\n",
    "\t\t\t\tfor idx, p in enumerate(pitches):\n",
    "\t\t\t\t\tif p.accidental is not None and \"#\" in p.name:\n",
    "\t\t\t\t\t\tpitches[idx] = pitch.Pitch(note_replacements[p.name], octave=p.octave)\n",
    "\t\t\t\tnew_cs = harmony.chordSymbolFromChord(chord.Chord(pitches))\n",
    "\t\t\t\tmeasure.append(new_cs.figure)\n",
    "\t\t\telse:\n",
    "\t\t\t\tif f\"add{cs.figure[0]}#\" in cs.figure:\n",
    "\t\t\t\t\tnc = cs.figure.replace(f\"add{cs.figure[0]}#\", \"b9\")\n",
    "\t\t\t\t\tmeasure.append(nc)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tmeasure.append(cs.figure)\n",
    "\tchord_measures.append(measure)\n",
    "\n",
    "note_measures = []\n",
    "for item in melody:\n",
    "\tmeasure = []\n",
    "\tfor n in item:\n",
    "\t\tif \"note.Note\" in str(n) or \"note.Rest\" in str(n):\n",
    "\t\t\tn_name = n.nameWithOctave if \"note.Note\" in str(n) else \"RS\"\n",
    "\t\t\tn_len = n.quarterLength\n",
    "\t\t\tmeasure.append([n_name, n_len])\n",
    "\tnote_measures.append(measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27aa52a",
   "metadata": {},
   "source": [
    "Let's take a look at measure one of `chord_measures` and `note_measures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b2b2d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cm']\n",
      "[['RS', 0.5], ['C5', 1.5], ['B4', 1.0], ['D5', 0.5], ['C5', 0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(chord_measures[0])\n",
    "print(note_measures[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb737d",
   "metadata": {},
   "source": [
    "## Sidestep, transposing\n",
    "\n",
    "Let's say we have a song in the key of Eb major. For training purposes, I want to only train and generate songs in C major. Can we transpose using `music21`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "23be7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "misty_path = \"data/misty.midi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a92e3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(play_midi(misty_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2bd3ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "misty = converter.parse(misty_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9a43f",
   "metadata": {},
   "source": [
    "Below, I show the route I went with to convert a `.midi` file to C major/A minor. For this purpose, I also exported it as a `.mid` file for playback so you can compare with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0663193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherwright/GitHub Projects/midi-ai/.venv/lib/python3.13/site-packages/music21/stream/base.py:3675: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    }
   ],
   "source": [
    "misty_key = misty.flat.getElementsByClass(key.KeySignature)[0]\n",
    "target_key = key.Key(\"C\") if misty_key.mode == \"major\" else key.Key(\"A\", \"minor\")\n",
    "i = interval.Interval(misty_key.tonic, target_key.tonic)\n",
    "\n",
    "misty_in_c = misty.transpose(i)\n",
    "# misty_in_c.write(\"midi\", fp=\"misty_in_c.mid\")\n",
    "# Audio(play_midi(\"misty_in_c.mid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3b028",
   "metadata": {},
   "source": [
    "So now that we know how to make sure we have consistent data for training, we can get back to where we were."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c50db3",
   "metadata": {},
   "source": [
    "## Loops -> functions\n",
    "\n",
    "Let's make functions out of our loops from before, as well as whatever else we can, to make things easier in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "739f799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_chords(chords_part):\n",
    "\tchord_measures = []\n",
    "\tfor item in chords_part:\n",
    "\t\tmeasure = []\n",
    "\t\tfor c in item:\n",
    "\t\t\tif \"chord.Chord\" in str(c):\n",
    "\t\t\t\tc_name = \",\".join([cn.name for cn in c.pitches])\n",
    "\t\t\t\tc_len = str(c.quarterLength)\n",
    "\t\t\t\tmeasure.append([c_name, c_len])\n",
    "\t\tchord_measures.append(measure)\n",
    "\treturn chord_measures\n",
    "\n",
    "def gen_notes(notes_part):\n",
    "\tnote_measures = []\n",
    "\tfor item in notes_part:\n",
    "\t\tmeasure = []\n",
    "\t\tfor n in item:\n",
    "\t\t\tif \"note.Note\" in str(n) or \"note.Rest\" in str(n):\n",
    "\t\t\t\tn_name = n.nameWithOctave if \"note.Note\" in str(n) else \"RS\"\n",
    "\t\t\t\tn_len = str(n.quarterLength)\n",
    "\t\t\t\tmeasure.append([n_name, n_len])\n",
    "\t\tnote_measures.append(measure)\n",
    "\treturn note_measures\n",
    "\n",
    "def parse_midi_in_c(file_path):\n",
    "\tsong = converter.parse(file_path)\n",
    "\toriginal_key = song.flatten().getElementsByClass(key.KeySignature)[0]\n",
    "\tif original_key.tonic == \"C\" and original_key.mode == \"major\":\n",
    "\t\treturn song\n",
    "\ttarget_key = key.Key(\"C\") if original_key.mode == \"major\" else key.Key(\"A\", \"minor\")\n",
    "\ti = interval.Interval(original_key.tonic, target_key.tonic)\n",
    "\n",
    "\tnew_song = song.transpose(i)\n",
    "\treturn new_song\n",
    "\n",
    "def gen_data(file_path):\n",
    "\tsong = parse_midi_in_c(file_path)\n",
    "\tchords = gen_chords(song.parts[0])\n",
    "\tnotes = gen_notes(song.parts[1])\n",
    "\treturn [chords, notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bf3338ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A4', '1.0'], ['A4', '1.0'], ['A4', '1.0'], ['A4', '0.5'], ['A4', '0.5']]\n"
     ]
    }
   ],
   "source": [
    "strange_fruit = gen_data(\"data/strange_fruit.midi\")\n",
    "print(strange_fruit[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4e8db",
   "metadata": {},
   "source": [
    "I'm going to now import `tqdm` to show our loading process for importing songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "01e1310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8a95a018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672c07186cca44649bb9bb2c717fef36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = Path(\"data\")\n",
    "files = [f for f in data_path.iterdir() if f.is_file() and f.suffix.lower() == \".midi\"]\n",
    "\n",
    "# all_songs = [\n",
    "#   # gen_data(fp) for fp in tqdm(files, unit=\"file\")\n",
    "#   [gen_data(fp) for fp in tqdm(files, unit=\"file\")]\n",
    "# ]\n",
    "\n",
    "all_songs = []\n",
    "\n",
    "for fp in tqdm(files, unit=\"file\"):\n",
    "  chords, notes = gen_data(fp)\n",
    "  song = [chords, notes]\n",
    "  all_songs.append(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fb4c9",
   "metadata": {},
   "source": [
    "## Tokenizing our data\n",
    "\n",
    "So now we have a list `all_songs` where the first index, `all_songs[0]`, `all_songs[1]`, etc. are the different songs and the second index, `all_songs[0][0]`, `all_songs[0][1]`, are the chord info and note info respectively.\n",
    "\n",
    "Now is the tricky part, what is going to be the best way to tokenize this data?\n",
    "\n",
    "First, what I'll need to do is figure out a good way to get my data into some easy-to-understand information for training. I'd really like to train my model on basically *each measure* to get my note probabilities *while being dependent on what the underlying chord is playing behind those notes* so that my model will be able to **generate melodies based on what chord is input**.\n",
    "\n",
    "Looks like I'll have to do some more reading and studying to figure out how to tackle this but I'm really happy with where I am at this point. I've taken my `.midi` files and parsed the notes and chords for each measure and created a list `all_songs` that holds every song in my `data/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
